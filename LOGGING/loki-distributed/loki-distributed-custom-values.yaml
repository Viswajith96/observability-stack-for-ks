# Loki Distributor
distributor:
  replicas: 2
  nodeSelector:
    karpenter.sh/provisioner-name: test-automation-cluster-provisioner-logging  
  tolerations:
    - key: "test-env/logging"
      operator: "Exists"
      effect: "NoSchedule"    
  resources:
    requests:
      cpu: "400m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "512Mi"  
  extraArgs:
  - --config.expand-env=true
  extraEnv:
  - name: S3_ACCESS_KEY_ID
    valueFrom:
      secretKeyRef:
        key: AWS_ACCESS_KEY_ID
        name: iam-loki-s3
  - name: S3_SECRET_KEY
    valueFrom:
      secretKeyRef:
        key: AWS_SECRET_ACCESS_KEY
        name: iam-loki-s3

# Loki Gateway
gateway:
  replicas: 2
  nodeSelector:
    karpenter.sh/provisioner-name: test-automation-cluster-provisioner-logging  
  tolerations:
    - key: "test-env/logging"
      operator: "Exists"
      effect: "NoSchedule"    
  resources:
    requests:
      cpu: "50m"
      memory: "64Mi"
    limits:
      cpu: "100m"
      memory: "128Mi"   
  affinity: |
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              {{- include "loki.gatewaySelectorLabels" . | nindent 10 }}
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "loki.gatewaySelectorLabels" . | nindent 12 }}
            topologyKey: failure-domain.beta.kubernetes.io/zone
  autoscaling:
    enabled: false
    maxReplicas: 3
    minReplicas: 1
    targetCPUUtilizationPercentage: 60
    targetMemoryUtilizationPercentage: null
  basicAuth:
    enabled: false
    htpasswd: '{{ htpasswd (required "''gateway.basicAuth.username'' is required"
      .Values.gateway.basicAuth.username) (required "''gateway.basicAuth.password''
      is required" .Values.gateway.basicAuth.password) }}'
    password: Pkc4jdMQi6He6HgVMpUC15vnu7dYFXjCklskl7nX439HkZ0aRSdUOqe5MNl
    username: zjHPWdvgN47hTc8mL6iTRTDJ6v4T27szBBXNIBWxBwL4rJCJxjeTxb1xgF
  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL
    readOnlyRootFilesystem: true
  deploymentStrategy:
    type: RollingUpdate
  dnsConfig: {}
  enabled: true
  extraArgs: []
  extraContainers: []
  extraEnv: []
  extraEnvFrom: []
  extraVolumeMounts: []
  extraVolumes: []
  image:
    pullPolicy: IfNotPresent
    registry: docker.io
    repository: nginxinc/nginx-unprivileged
    tag: 1.19-alpine
  ingress:
    annotations:
      cert-manager.io/acme-challenge-type: http01
      cert-manager.io/cluster-issuer: letsencrypt-issuer
      ingress.kubernetes.io/ssl-redirect: "false"
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/whitelist-source-range: 35.174.225.122/32, 3.111.235.200/32, 3.216.25.235/32
    enabled: true
    hosts:
    - host: production-logs.test.cloud
      paths:
      - path: /
        pathType: Prefix
    ingressClassName: ""
    tls:
    - hosts:
      - production-logs.test.cloud
      secretName: production-logs-tls
  livenessProbe:
    httpGet:
      path: /
      port: http
    initialDelaySeconds: 30
  nginxConfig:
    file: |
      worker_processes  5;  ## Default: 1
      error_log  /dev/stderr;
      pid        /tmp/nginx.pid;
      worker_rlimit_nofile 8192;

      events {
        worker_connections  4096;  ## Default: 1024
      }

      http {
        client_body_temp_path /tmp/client_temp;
        proxy_temp_path       /tmp/proxy_temp_path;
        fastcgi_temp_path     /tmp/fastcgi_temp;
        uwsgi_temp_path       /tmp/uwsgi_temp;
        scgi_temp_path        /tmp/scgi_temp;

        proxy_http_version    1.1;

        default_type application/octet-stream;
        log_format   {{ .Values.gateway.nginxConfig.logFormat }}

        {{- if .Values.gateway.verboseLogging }}
        access_log   /dev/stderr  main;
        {{- else }}

        map $status $loggable {
          ~^[23]  0;
          default 1;
        }
        access_log   /dev/stderr  main  if=$loggable;
        {{- end }}

        sendfile     on;
        tcp_nopush   on;
        {{- if .Values.gateway.nginxConfig.resolver }}
        resolver {{ .Values.gateway.nginxConfig.resolver }};
        {{- else }}
        resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};
        {{- end }}

        {{- with .Values.gateway.nginxConfig.httpSnippet }}
        {{ . | nindent 2 }}
        {{- end }}

        server {
          listen             8080;

          {{- if .Values.gateway.basicAuth.enabled }}
          auth_basic           "Loki";
          auth_basic_user_file /etc/nginx/secrets/.htpasswd;
          {{- end }}

          location = / {
            return 200 'OK';
            auth_basic off;
          }

          location = /api/prom/push {
            set $api_prom_push_backend http://{{ include "loki.distributorFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass       $api_prom_push_backend:3100$request_uri;
          }

          location = /api/prom/tail {
            set $api_prom_tail_backend http://{{ include "loki.querierFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass       $api_prom_tail_backend:3100$request_uri;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
          }

          # Ruler
          location ~ /prometheus/api/v1/alerts.* {
            proxy_pass       http://{{ include "loki.rulerFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
          }
          location ~ /prometheus/api/v1/rules.* {
            proxy_pass       http://{{ include "loki.rulerFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
          }
          location ~ /api/prom/rules.* {
            proxy_pass       http://{{ include "loki.rulerFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
          }
          location ~ /api/prom/alerts.* {
            proxy_pass       http://{{ include "loki.rulerFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
          }

          location ~ /api/prom/.* {
            set $api_prom_backend http://{{ include "loki.queryFrontendFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass       $api_prom_backend:3100$request_uri;
          }

          location = /loki/api/v1/push {
            set $loki_api_v1_push_backend http://{{ include "loki.distributorFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass       $loki_api_v1_push_backend:3100$request_uri;
          }

          location = /loki/api/v1/tail {
            set $loki_api_v1_tail_backend http://{{ include "loki.querierFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass       $loki_api_v1_tail_backend:3100$request_uri;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
          }

          location ~ /loki/api/.* {
            set $loki_api_backend http://{{ include "loki.queryFrontendFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass       $loki_api_backend:3100$request_uri;
          }

          {{- with .Values.gateway.nginxConfig.serverSnippet }}
          {{ . | nindent 4 }}
          {{- end }}
        }
      }
    httpSnippet: ""
    logFormat: |-
      main '$remote_addr - $remote_user [$time_local]  $status '
              '"$request" $body_bytes_sent "$http_referer" '
              '"$http_user_agent" "$http_x_forwarded_for"';
    resolver: ""
    serverSnippet: ""
  nodeSelector: {}
  podAnnotations: {}
  podLabels: {}
  podSecurityContext:
    fsGroup: 101
    runAsGroup: 101
    runAsNonRoot: true
    runAsUser: 101
  priorityClassName: null
  readinessProbe:
    httpGet:
      path: /
      port: http
    initialDelaySeconds: 15
    timeoutSeconds: 1
  replicas: 1
  resources: {}
  service:
    annotations: {}
    clusterIP: null
    labels: {}
    loadBalancerIP: null
    loadBalancerSourceRanges: []
    nodePort: null
    port: 80
    type: ClusterIP
  terminationGracePeriodSeconds: 30
  tolerations: []
  verboseLogging: true

# Loki Ingester  
ingester:
  nodeSelector:
    karpenter.sh/provisioner-name: test-automation-cluster-provisioner-logging    
  tolerations:
    - key: "test-env/logging"
      operator: "Exists"
      effect: "NoSchedule"                                                 
  resources:
    requests:
      cpu: "800m"
      memory: "8192Mi"
    limits:
      cpu: "1000m"
      memory: "10240Mi"     
  extraArgs:
  - --config.expand-env=true
  extraEnv:
  - name: S3_ACCESS_KEY_ID
    valueFrom:
      secretKeyRef:
        key: AWS_ACCESS_KEY_ID
        name: iam-loki-s3
  - name: S3_SECRET_KEY
    valueFrom:
      secretKeyRef:
        key: AWS_SECRET_ACCESS_KEY
        name: iam-loki-s3
  replicas: 3

# Loki   
loki:
  nodeSelector:
    karpenter.sh/provisioner-name: test-automation-cluster-provisioner-logging  
  tolerations:
    - key: "test-env/logging"
      operator: "Exists"
      effect: "NoSchedule"    
  structuredConfig:
    ingester:
      chunk_idle_period: 1h
      chunk_target_size: 15360000
      lifecycler:
        ring:
          kvstore:
            store: memberlist
          replication_factor: 3
      max_chunk_age: 1h
      max_transfer_retries: 0
    schema_config:
      configs:
      - from: "2020-09-07"
        index:
          period: 24h
          prefix: loki_index_
        object_store: aws
        schema: v11
        store: boltdb-shipper
    storage_config:
      aws:
        access_key_id: ${S3_ACCESS_KEY_ID}
        bucketnames: test-production-loki-logs
        s3: s3://us-east-1
        secret_access_key: ${S3_SECRET_KEY}
      boltdb_shipper:
        shared_store: s3

# Loki Querier        
querier:
  replicas: 2
  nodeSelector:
    karpenter.sh/provisioner-name: test-automation-cluster-provisioner-logging  
  tolerations:
    - key: "test-env/logging"
      operator: "Exists"
      effect: "NoSchedule"    
  resources:
    requests:
      cpu: "800m"
      memory: "2048Mi"
    limits:
      cpu: "1000m"
      memory: "4096Mi"  
  extraArgs:
  - --config.expand-env=true
  extraEnv:
  - name: S3_ACCESS_KEY_ID
    valueFrom:
      secretKeyRef:
        key: AWS_ACCESS_KEY_ID
        name: iam-loki-s3
  - name: S3_SECRET_KEY
    valueFrom:
      secretKeyRef:
        key: AWS_SECRET_ACCESS_KEY
        name: iam-loki-s3
# Loki Querier  Frontend        
queryFrontend:
  replicas: 2
  nodeSelector:
    karpenter.sh/provisioner-name: test-automation-cluster-provisioner-logging  
  tolerations:
    - key: "test-env/logging"
      operator: "Exists"
      effect: "NoSchedule"    
  resources:
    requests:
      cpu: "100m"
      memory: "128Mi"
    limits:
      cpu: "200m"
      memory: "512Mi"
  extraArgs:
  - --config.expand-env=true
  extraEnv:
  - name: S3_ACCESS_KEY_ID
    valueFrom:
      secretKeyRef:
        key: AWS_ACCESS_KEY_ID
        name: iam-loki-s3
  - name: S3_SECRET_KEY
    valueFrom:
      secretKeyRef:
        key: AWS_SECRET_ACCESS_KEY
        name: iam-loki-s3


